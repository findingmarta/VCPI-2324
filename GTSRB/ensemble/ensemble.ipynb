{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import collections \n",
    "\n",
    "# importing a module with utilities for displaying stats and data\n",
    "import sys\n",
    "sys.path.insert(1, '../util')\n",
    "import vcpi_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "\n",
    "    # sets the model in evaluation mode.\n",
    "    # although our model does not have layers which behave differently during training and evaluation\n",
    "    # this is a good practice as the models architecture may change in the future\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    \n",
    "    for i, (images, targets) in enumerate(data_loader):\n",
    "         \n",
    "        # forward pass, compute the output of the model for the current batch\n",
    "        outputs = model(images.to(device))\n",
    "\n",
    "        # \"max\" returns a namedtuple (values, indices) where values is the maximum \n",
    "        # value of each row of the input tensor in the given dimension dim; \n",
    "        # indices is the index location of each maximum value found (argmax).\n",
    "        # the argmax effectively provides the predicted class number        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        correct += (preds.cpu() == targets).sum()\n",
    "\n",
    "    return (correct / len(data_loader.dataset)).item()\n",
    "\n",
    "\n",
    "class Conv(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(3, 128, 3) \n",
    "        self.bn1 = torch.nn.BatchNorm2d(128)    \n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.drop1 = torch.nn.Dropout2d(0.3)\n",
    "\n",
    "        self.conv2 = torch.nn.Conv2d(128, 256, 3)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(256)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.drop2 = torch.nn.Dropout2d(0.3)\n",
    "\n",
    "        self.maxpool1 = torch.nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv3 = torch.nn.Conv2d(256, 512, 3)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(512)        \n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "        self.drop3 = torch.nn.Dropout2d(0.3)\n",
    "\n",
    "        self.conv4 = torch.nn.Conv2d(512, 512, 3)\n",
    "        self.bn4 = torch.nn.BatchNorm2d(512)\n",
    "        self.relu4 = torch.nn.ReLU()\n",
    "        self.drop4 = torch.nn.Dropout2d(0.3)\n",
    "\n",
    "        self.maxpool2 = torch.nn.MaxPool2d(2)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(12800, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # input = (batch_size, 3, 32, 32)\n",
    "        x = self.conv1(x)                  # output = (batch_size, 128, 30, 30) \n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.conv2(x)                  # output = (batch_size, 256, 28, 28)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = self.maxpool1(x)               # output = (batch_size, 256, 14, 14)\n",
    "\n",
    "        x = self.conv3(x)                  # output = (batch_size, 512, 12, 12)                     \n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.drop3(x)\n",
    "\n",
    "        x = self.conv4(x)                  # output = (batch_size, 512, 10, 10)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.drop4(x)\n",
    "\n",
    "        x = self.maxpool2(x)               # output = (batch_size, 512, 5, 5)     512 x 5 x 5 = 12800\n",
    "\n",
    "        x = torch.flatten(x, 1)             \n",
    "        x = self.fc1(x)                                                                                     \n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathTest = '../test_images'\n",
    "\n",
    "# Batch size - It can influence speed and generalization, not necessarily in the same direction. \n",
    "# There is no golden rule for the batch size but 32 is a commom number to start with.\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "IMAGE_SIZE = 32\n",
    "EPOCHS = 40\n",
    "\n",
    "NUM_MODELS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = v2.Compose([\n",
    "    v2.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    v2.RandomAutocontrast(p=1.0),          # Apply autocontrast to the image\n",
    "    v2.ToImage(),                          # Convert to tensor\n",
    "    v2.ToDtype(torch.float32, scale=True)  # Normalize to [0,1]\n",
    "])\n",
    "\n",
    "# No shuffle is required for the test set, also the batch size can be completely different\n",
    "test_set = torchvision.datasets.ImageFolder(root=pathTest, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv(\n",
       "  (conv1): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (drop1): Dropout2d(p=0.3, inplace=False)\n",
       "  (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (drop2): Dropout2d(p=0.3, inplace=False)\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (drop3): Dropout2d(p=0.3, inplace=False)\n",
       "  (conv4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu4): ReLU()\n",
       "  (drop4): Dropout2d(p=0.3, inplace=False)\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=12800, out_features=43, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Conv(len(test_set.classes))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "acc = []\n",
    "\n",
    "# Load the five best models\n",
    "for i in range(NUM_MODELS):\n",
    "    reload = torch.load(f'best_model_{i}.pt')\n",
    "    model = Conv(len(test_set.classes))                                           \n",
    "    model.load_state_dict(reload['model'])\n",
    "    model.to(device)\n",
    "    models.append(model)\n",
    "    acc.append(evaluate(models[i], test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9936975479125977 0.9972288012504578 0.9831354022026062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.99, 1.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = [i for i in range(NUM_MODELS)]\n",
    "  \n",
    "plt.bar(index, acc)\n",
    "print(np.mean(acc), np.max(acc), np.min(acc))\n",
    "plt.ylim(0.99,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_logits_and_preds(models):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = [[] for _ in range(NUM_MODELS)]\n",
    "        labels = []\n",
    "\n",
    "        for images, labs in test_loader:\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels.extend(labs)\n",
    "            \n",
    "            for i in range(NUM_MODELS):\n",
    "                logits[i].extend(models[i](images).cpu())\n",
    "    return labels, logits\n",
    "\n",
    "labels, logits = get_labels_logits_and_preds(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 8, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "preds = [[] for _ in range(len(labels)) ]\n",
    "\n",
    "for index in range(len(labels)):\n",
    "    preds[index] = [np.argmax(logits[m][index].cpu().numpy()) for m in range(NUM_MODELS)]\n",
    "\n",
    "print(preds[4000]) # 4000 is the index of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "def get_class_from_sum_of_logits(logits):\n",
    "\n",
    "    sum_logits = []\n",
    "\n",
    "    for i in range(len(logits[0])):\n",
    "\n",
    "        log = logits[0][i]\n",
    "        for m in range(1, NUM_MODELS):\n",
    "            log = np.add(log, logits[m][i])\n",
    "        sum_logits.append(np.argmax(log))\n",
    "    return(sum_logits)\n",
    "    \n",
    "class_logits = get_class_from_sum_of_logits(logits)\n",
    "print(class_logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  12630\n",
      "All correct:  12345\n",
      "All incorrect:  3\n",
      "Majority correct:  252\n",
      "Tie Vote:  11\n",
      "Majority Wrong:  19\n",
      "Percentage right:  0.9973871733966746\n"
     ]
    }
   ],
   "source": [
    "def VotingEnsemble(labels, class_preds, class_logits):\n",
    "\n",
    "    all_correct = 0\n",
    "    all_incorrect = 0\n",
    "    maj_vote = 0\n",
    "    maj_wrong = 0\n",
    "    tie = 0\n",
    "    count = 0\n",
    "\n",
    "    for k in range(len(labels)):\n",
    "\n",
    "        counter = collections.Counter(class_preds[k])\n",
    "        if len(counter) == 1:\n",
    "            if counter.most_common(1)[0][0] == labels[k]:\n",
    "                all_correct += 1\n",
    "            else:\n",
    "                all_incorrect += 1\n",
    "        else:\n",
    "            aux = counter.most_common(2)\n",
    "            if aux[0][1] > aux[1][1] and aux[0][0] == labels[k]:\n",
    "                maj_vote += 1\n",
    "            if aux[0][1] > aux[1][1] and aux[0][0] != labels[k]:\n",
    "                maj_wrong += 1\n",
    "            elif aux[0][1] == aux[1][1]:\n",
    "                tie += 1\n",
    "\n",
    "        count += 1 \n",
    "        \n",
    "    return [count, all_correct, all_incorrect, maj_vote, tie, maj_wrong]\n",
    "    \n",
    "    \n",
    "res = VotingEnsemble(labels, preds, class_logits)\n",
    "print('total: ', res[0])\n",
    "print('All correct: ', res[1])\n",
    "print('All incorrect: ', res[2])\n",
    "print('Majority correct: ', res[3])\n",
    "print('Tie Vote: ', res[4])\n",
    "print('Majority Wrong: ', res[5])\n",
    "print('Percentage right: ', (res[1]+res[3])/res[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
